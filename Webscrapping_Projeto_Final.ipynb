{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium"
      ],
      "metadata": {
        "id": "7ytC7wzsLGGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4"
      ],
      "metadata": {
        "id": "J83Mby2_LOmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Projeto Final - Big Data e computação em nuvem**\n",
        "### Webscrapping do site da [National Centers for Environmental Information (NCEI)](https://https://www.ncei.noaa.gov/)\n",
        "\n",
        "Para a realização do projeto, foi realizado a raspagem de dados do site nacional do estados unidos para a obtenção de dados geográficos, relacionado ao clima e precipitação.\n",
        "\n",
        "No site, há dados de clima de todos os 49 estados americanos desde 1895. Para a utilização do projeto, foram obtidos os dados de precipitação e temperatura média do perído de 2009 à 2018."
      ],
      "metadata": {
        "id": "DlXYIbnodSBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython\n",
        "import itertools\n",
        "import numpy as np\n",
        "from selenium import webdriver\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "import re"
      ],
      "metadata": {
        "id": "NXLDQXhKLV9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Criação do driver**\n",
        "Para este projeto será utilizado o *chromedriver* como o agente de consulta do site, para realizar a raspagem de dados."
      ],
      "metadata": {
        "id": "lr4cPJ0xft4R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-v9CalymLEVU"
      },
      "outputs": [],
      "source": [
        "#Lista de users agents para evitar o bloqueio do driver\n",
        "user_agents_list = [\n",
        "    'Mozilla/5.0 (iPad; CPU OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148',\n",
        "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.83 Safari/537.36',\n",
        "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.51 Safari/537.36'\n",
        "]\n",
        "\n",
        "#Argumentos do driver\n",
        "options = Options()\n",
        "options.add_argument(\"--headless\")\n",
        "options.add_argument(\"--no-sandbox\")\n",
        "options.add_argument('--disable-blink-features=AutomationControlled')\n",
        "options.headless = True\n",
        "\n",
        "#Escolha do user_agent\n",
        "user_agent = {'User-Agent': random.choice(user_agents_list)}\n",
        "\n",
        "options.add_argument('user-agent={0}'.format(user_agent))\n",
        "\n",
        "driver = webdriver.Chrome(options=options)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Funções de *scrapping***\n",
        "\n",
        "Antes de realizar a raspagem de dados, foram criadas duas funções para facilitar o processo, além de melhorar a compreensão dos códigos.\n",
        "\n",
        "As funções criadas foram:\n",
        "\n",
        "- *find_state*: Recebe uma variável do BeautifulSoup e realiza a procura do campo que apresenta o estado em que se apresenta o site. A função retorna uma string com o nome do estado.\n",
        "\n",
        "- *find_info*: Recebe uma variável do BeautifulSoup e realiza a procura das linhas da tabela apresentada no site onde resgata o ano, o mês e o dado apresentado na tabela. A função retorna uma lista de todos Anos e meses com os dados desejados."
      ],
      "metadata": {
        "id": "2EFlu4kJgDTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_state(soup):\n",
        "  \"\"\" Encontra o estado em que se apresenta as informações\n",
        "  Retorna uma string do estado\"\"\"\n",
        "\n",
        "  form = soup.find(\"div\", id=\"form-sections\")\n",
        "  form_loc = form.find(\"select\", id=\"location\")\n",
        "  state = form_loc.find(\"option\", selected=\"selected\").text\n",
        "\n",
        "  return state\n",
        "\n",
        "def find_info(soup):\n",
        "  \"\"\" Encontra o parâmetro presente na tabela do site, assim como mês e ano da observação\n",
        "  Retorna uma lista com 3 valores por observação ([Ano, Mês, Parâmetro])\n",
        "  Obs: O parâmetro é convertido para float\n",
        "  \"\"\"\n",
        "  info = []\n",
        "\n",
        "  table = soup.find(\"tbody\")\n",
        "  rows = table.find_all(\"tr\", role=\"row\")\n",
        "\n",
        "  for row in rows:\n",
        "    date = row.find_all(\"td\")[0].text.split()\n",
        "    mth = date[0]\n",
        "    year = date[1]\n",
        "\n",
        "    data = row.find_all(\"td\")[1].text\n",
        "    data_float = float(re.findall(r\"\\d+.\\d+\", data)[0])\n",
        "\n",
        "    info.append([year, mth, data_float])\n",
        "\n",
        "  return info"
      ],
      "metadata": {
        "id": "Okt_MyEOTNoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Webscrapping**\n",
        "Com as funções que realizam a aquisição dos dados e o driver criado, é feito a operação de webscrapping.\n",
        "\n",
        "O site da NCEI é um site dinâmico, ou seja, alguns recursos dele são criados dependendo de algum parâmetro dentro do próprio site. No caso o objeto dinâmico seria a tabela, que muda de acordo com a mudança de parâmetros do site, que seria o objeto que seria adquirido.\n",
        "\n"
      ],
      "metadata": {
        "id": "fhVlBZ1Qh-xf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Número de estados\n",
        "len_states = 49\n",
        "\n",
        "#Parâmetros que vão ser adquiridos\n",
        "param = [\"tavg\", \"pcp\"]\n",
        "\n",
        "#Criação do dataframe\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for state_num in range(1,len_states + 1,1):\n",
        "\n",
        "  #Condição para incluir o Alaska\n",
        "  if state_num == 49:\n",
        "    state_num = 50\n",
        "\n",
        "  for p in param:\n",
        "\n",
        "        #Criação de um dataframe temporário para o loop\n",
        "        df_tmp = pd.DataFrame()\n",
        "\n",
        "        time.sleep(1)\n",
        "\n",
        "        #Url da página com os\n",
        "        page_url = \"https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/statewide/time-series/\"+str(state_num)+\"/\"+p+\"/all/10/2009-2018\"\n",
        "\n",
        "        driver.get(page_url)\n",
        "        driver.implicitly_wait(30)\n",
        "\n",
        "        # Espera o elemento aparecer na página\n",
        "        element = WebDriverWait(driver, 60).until(\n",
        "            EC.presence_of_element_located((By.XPATH, \"/html/body/main/div/div/div[6]/table/tbody/tr[1]\"))\n",
        "        )\n",
        "\n",
        "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "\n",
        "        state = find_state(soup)\n",
        "        info = find_info(soup)\n",
        "\n",
        "        df_tmp[\"Year\"] = [i[0] for i in info]\n",
        "        df_tmp[\"Month\"] = [i[1] for i in info]\n",
        "        df_tmp[p] = [i[2] for i in info]\n",
        "        df_tmp[\"State\"] = state\n",
        "\n",
        "        #União do dataframe temporário do loop com o dataframe principal\n",
        "        df = pd.concat([df, df_tmp], ignore_index=True, axis=0)\n",
        "\n",
        "df = df.groupby(['State', 'Year', 'Month']).apply(lambda group: group.ffill().bfill())"
      ],
      "metadata": {
        "id": "J_o6Oo4MLRnW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09555071-f7c4-456f-b64f-ed8b4c4160c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-113-2b7a6722c76d>:40: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
            "To preserve the previous behavior, use\n",
            "\n",
            "\t>>> .groupby(..., group_keys=False)\n",
            "\n",
            "To adopt the future behavior and silence this warning, use \n",
            "\n",
            "\t>>> .groupby(..., group_keys=True)\n",
            "  df = df.groupby(['State', 'Year', 'Month']).apply(lambda group: group.ffill().bfill())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exportação dos dados**\n",
        "Feita a operação de webscrapping, o dataframe é exportado para um arquivo csv e assim pode ser utilizado no projeto."
      ],
      "metadata": {
        "id": "OiWbWPsGjii6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "#Reordenação das colunas\n",
        "df = df[['State', 'Year', 'Month', 'tavg', 'pcp']]\n",
        "\n",
        "#Conversão do dataframe para csv e download\n",
        "df.to_csv('df_weather.csv', index=False)\n",
        "files.download('df_weather.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "RUGUwMOVXwjg",
        "outputId": "de64a27b-61ba-4228-d4e7-fea4f3f2fb36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6562ae70-ab38-4f26-ba23-8afe733843e8\", \"df_weather.csv\", 371128)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}